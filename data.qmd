# Data

## Technical Description

The data we are analyzing in this project is provided by the Public-Safety website of the Montgomery County, MD [link](https://data.montgomerycountymd.gov/Public-Safety/Crash-Reporting-Incidents-Data/bhju-22kf){.class-to-style-as-link}

The data is supplied by all the police departments of this county. Moreover, the description of the data set mentions that :
*"Please note that these collision reports are based on preliminary information supplied to the Police Department by the reporting parties. Therefore, the collision data available on this web page may reflect:

- Information not yet verified by further investigation
- Information that may include verified and unverified collision data
- Preliminary collision classifications may be changed at a later date based upon further investigation
- Information may include mechanical or human error"*

The above impose some limitations using the data.

As outlined in the dataset description, the data undergoes **weekly updates**. The source dataset comprises 95.6k rows and 44 columns, each representing a distinct car collision. The dataset encompasses geographical, numerical, categorical, and date-time information.

The dataset, **which we acquired on November 22, 2023**, is accessible as a .csv file in the GitHub repository under the 'data' directory. For the purposes of this project, we conducted an analysis of the data up until that date. It is important for readers of this project to note that while our work can be replicated and extended with updated information, there may be a necessity for additional preprocessing to address novel issues that may have arisen after November 2023. Furthermore, some conclusions might need to be revisited and updated based on the latest data.

To reiterate, all analyses presented in this work are based on data available until November 22, 2023.



## Research Plan
In this course, we were introduced to various types of plots, and we intend to leverage these visualizations to uncover factors contributing to a higher number of collisions (car accidents) in the Montgomery county.

As sated in the introduction, we divide the analysis into **three parts**:

Firstly, our exploration will focus **time-series analysis** of the "Crash Date/Time" to identify potential trends, seasonality and key insights in the collision data. We aim to investigate whether there is a fluctuation in the number of accidents during specific months, days of the week, or hours of the day as well as compare these between the different years. Additionally, we are interested in understanding if holidays or weeks with holidays exhibit a distinct pattern in terms of accident frequency. To enable that analysis, we downloaded "holiday" data which presents the american holiday-dates in each year, and we merge this information with the data we exported from the public safety website of the Montgomery county.

Our second area of inquiry addresses the pressing issue of **hit-and-run accidents**. Utilizing the available data, we plan to examine the correlation between various features and the "Hit-Run" column. For example, We use mosaic plots for exploring relationships, associations and patterns within "Hit-Run" variable to other categorical variables. Insights gained from this analysis could contribute to a better understanding of the Hit and Run type of accident problem and provide recommendations for policymakers to mitigate it.

Lastly, we will employ **geographical**, or **spatial**, data to explore the relationship between accident density and geographical attributes. Specifically, we aim to identify roads with the highest concentration of accidents and analyze common attributes among them. This analysis may uncover patterns or characteristics that could inform recommendations for reducing accidents in these locations. Our approach involves utilizing spatial data to gain deeper insights into both "regular" accidents and "Hit and Run" incidents.

Overall, our goal is to identify combinations of features that could elevate the risk of collisions. This information can then provide policymakers with valuable insights to implement meaningful changes or restrictions on the roads.



## Missing value analysis

```{r setup, include=FALSE}
#Import the libraries

knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dplyr)

library(devtools)
#library(ggthemr)
#ggthemr("flat")
#install.packages("remotes")
remotes::install_github("jtr13/redav")
library("redav")

```



```{r}
data_crash <- read_csv("data/Crash_Reporting_-_Incidents_Data.csv", col_types = cols('Local Case Number' = col_character())) # the 2nd column contains string and not number that's why we specify it.
data_crash <- rename(data_crash, "Hit_Run" = `Hit/Run`) 
data_crash <- rename(data_crash,"Crash_Date_Time" = `Crash Date/Time`)

#rename("Crash_Date_Time" = `Crash_Date/Time`)

### Preprocess:
new_col_names <- colnames(data_crash)
# Replace spaces and dashes with underscores
new_col_names <- gsub(" ", "_", new_col_names)
new_col_names <- gsub("-", "_", new_col_names)
colnames(data_crash) <- new_col_names

data_crash |> head()
```

```{r}
# Function to replace "N/A" with NA in a vector
replace_na <- function(x) {
  x[x == "N/A"] <- NA
  return(x)
}

# Apply the function to each column of the data_crash frame
data_crash <- as_tibble(lapply(data_crash, replace_na))
```


## Analyzing NAs


```{r}
## PLot the number of NAs for each column

b <- apply(data_crash, 2, function(c) sum(is.na(c)))
tibble_nas <- tibble(column_name = names(b), number_of_NA = b)
tibble_nas

ggplot(tibble_nas |> filter(number_of_NA >0 ))+
  geom_col(mapping = aes(y=reorder(column_name,number_of_NA), x = number_of_NA)) +
  labs(title = "Number of Nas in the data columns",
       subtitle = "Showing only columns with number of NAs > 0",
       y = 'Column name')


```



```{r}

paste0("Number of columns with at least 1 NA = ", tibble_nas |> filter(number_of_NA > 0) |> nrow())

paste0("Number of columns without any NAs = ", tibble_nas |> filter(number_of_NA == 0) |> nrow())

```

There are 9 columns with "too many" NAs (more than 45% of the rows)
```{r}
columns_with_too_many_nas <- tibble_nas|> filter(number_of_NA>0) |> arrange(number_of_NA) |> filter(number_of_NA > 45000)
columns_with_too_many_nas
```
We will remove those columns :

```{r}
data_crash <- dplyr::select(data_crash,-as.vector(columns_with_too_many_nas$column_name))
```


+ Further, there are many columns with NA values that mostly overlap each other (if there is NA in one of those columns it appears also in the other ones), The following code reflect that fact --> if we filter the NAs from the column: Cross-Street Type --> and we can observe that the number of columns with NAs reduced significantly.


```{r}
filtered_data_crash <- data_crash[!is.na(data_crash$Cross_Street_Type),]


b <- apply(filtered_data_crash, 2, function(c) sum(is.na(c)))
tibble_nas <- tibble(column_name = names(b), number_of_NA = b)
tibble_nas_filtered_data_crash <- tibble_nas


ggplot(tibble_nas |> filter(number_of_NA >0 ))+
  geom_col(mapping = aes(y=reorder(column_name,number_of_NA), x = number_of_NA)) +
  labs(title = "Number of Nas in the data_crash columns",
       subtitle = "Showing only columns with number of NAs > 0",
       y = 'Column name')
```

For now we won't deal with those NAs. Later in the project we would choose the specific columns to analyze together and then we will decide on the best method to deal with the NAs in those columns.

## Repartition of the NAs in between columns

We want to get a list of columns that has the same "NA" pattern:

```{r}
b <- apply(data_crash, 2, function(c) sum(is.na(c)))
tibble_nas_data_crash <- tibble(column_name = names(b), number_of_NA = b)

#Compare the 2 lists, the one with all the column with less that 1000 NAs and the lists with the same conditions but after removing all the NAs row in the column "Cross-Street Type"
a <- tibble_nas_data_crash |> filter(number_of_NA<1000)
b <- tibble_nas_filtered_data_crash |> filter(number_of_NA<1000)

# Get the name of the column which are not in common in the 2 previous list
difference_both_ways <- union(setdiff(a$column_name, b$column_name), setdiff(b$column_name, a$column_name))
print(difference_both_ways)
```

We want to use the `plot_missing` function for `redav` package to see the NAs pattern regarding the columns that we suspect has NAs in the same rows:

```{r}
redav::plot_missing(data_crash |>dplyr::select(difference_both_ways,))
```

Using the `plot_missing` function, we can clearly see that those columns indeed has NAs in the same rows. We can conclude that their NAs are correlated.

Further, we will take one of those columns: `Cross_Street_Type` and use it as a representative of the 12 columns group and then see if there is any pattern between the NAs of the other columns:

```{r}
b <- apply(data_crash, 2, function(c) sum(is.na(c)))
tibble_nas <- tibble(column_name = names(b), number_of_NA = b)


columns_with_na <- tibble_nas |> filter(number_of_NA>500) 

columns_with_na <- columns_with_na$column_name

differences <- union(setdiff(columns_with_na, difference_both_ways), setdiff(difference_both_ways, columns_with_na))

differences <- c(differences, "Cross_Street_Type")
print(differences)

redav::plot_missing(data_crash |>dplyr::select(differences,))
```

One can see that there is no common pattern between all the NAs for these 10 columns.

In conclusion, for all the columns in the array 'difference_both_ways' we can deal with the NAs in the same way in the future. However, for the other columns which still has a lot of NAs like Weather or Traffic Control, we need to be more careful when removing the NAs because we could end up with very few rows in the data set. We will take care of them on a case-by-case basis.



```{r}
write_csv2(data_crash, "data/data_crash.csv")
```

